\documentclass[11pt]{beamer}
\title[Digital Adaptive Circuits for Learning Systems]{Sound Event Detection con la tecnica del ``few-shot learning''}
\author{Matteo Orlandini e Jacopo Pagliuca}
\date{\today}
\institute[UnivPM]{Università Politecnica delle Marche}
%\logo{\includegraphics[width=15mm]{Immagini/univpmlogo}}
\titlegraphic{
\includegraphics[width=2cm]{Immagini/univpmlogo}
}
\usepackage[english, italian]{babel} %l?ultima lingua dichiarata è la lingua principale del documento
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{animate} %per le gif 
\usepackage{xcolor}
\usepackage{listings}

\lstset{% general command to set parameter(s)
	backgroundcolor=\color{white},
	%backgroundcolor=\color{White}, % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
	basicstyle=\small,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,
	captionpos=b,                    % sets the caption-position to bottom
	%extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	keywordstyle=\color{blue}\bfseries, 	% blue keywords
	language=C,
	classoffset=0,
	morekeywords={bool,ifdef,ifndef,endif},
	keywordstyle=\color{blue}\bfseries,
	classoffset=1,
	morekeywords={__LOG_INIT,__LOG,adv_start,bearer_adtype_add,advertiser_enable,advertiser_packet_alloc,memcpy,advertiser_packet_send,ERROR_CHECK,mesh_provisionee_prov_start,adv_start,mesh_app_uuid_print,nrf_mesh_configure_device_uuid_get,mesh_stack_start,nrf_mesh_enable,provisioning_complete_cb,dsm_local_unicast_addresses_get,hal_led_blink_stop,hal_led_mask_set,hal_led_mask_set,device_identification_start_cb,hal_led_blink_ms,LED_BLINK_ATTENTION_COUNT,provisioning_aborted_cb,hal_led_blink_stop,nrf_mesh_rx_cb_set,adv_init,LEDS_OFF,sprintf,__LOG_XB,LEDS_ON,advertiser_instance_init,power_management_init,nrf_pwr_mgmt_init,APP_ERROR_CHECK,MSEC_TO_UNITS,memset,ble_advdata_encode,sd_ble_gap_adv_set_configure,sd_ble_gap_adv_start,bsp_indication_set,gap_params_init,conn_params_init,nrf_sdh_enable_request,nrf_sdh_ble_default_cfg_set,nrf_sdh_ble_enable,ble_stack_init,advertising_init,advertising_start,initialize,start,gatt_init,nrf_ble_gatt_init,NRF_BLE_GATT_DEF,hal_led_pin_set,nrf_delay_ms,NRF_MESH_ERROR_CHECK,sd_ble_gatts_characteristic_add,sd_ble_gatts_characteristic_add,BLE_GAP_CONN_SEC_MODE_SET_OPEN,advertiser_packet_discard,defined,on_es_evt,bsp_board_led_invert,bsp_board_led_on,bsp_board_led_off,nrf_ble_es_init,nrf_mesh_on_sd_evt, nrf_ble_es_on_start_connectable_advertising},
	keywordstyle=\color{black}\bfseries,
	classoffset=0,
	identifierstyle=,           % nothing happens
	commentstyle=\color{green}, % green comments
	stringstyle=\color{red},  %\ttfamily    % typewriter type for strings
	showstringspaces=false,	% no special string spaces
	numberbychapter=true,
	tabsize=2,
	columns=flexile,
	keepspaces=false,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	frame=lines,
	literate=
	{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
	{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
	{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
	{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
	{ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
	{Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
	{â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
	{Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
	{Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
	{?}{{\oe}}1 {?}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
	{?}{{\H{u}}}1 {?}{{\H{U}}}1 {?}{{\H{o}}}1 {?}{{\H{O}}}1
	{ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
	{?}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
	{»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}  

\usetheme{Antibes}
\usecolortheme{default} %https://hartwork.org/beamer-theme-matrix/
%\useoutertheme[right,color=red]{sidebar} %{infolines}
%\setbeamertemplate{sidebar canvas right}[vertical shading][top=red,bottom=white]
\setbeamercovered{dynamic}

\begin{document}
	\begin{frame}
		\maketitle
	\end{frame}

\begin{frame}
	\frametitle{Contenuti}
	\tableofcontents
\end{frame}

\section{Introduzione}
\begin{frame}
	\frametitle{Bluetooth Low Energy}
	\begin{columns}
		\column{0.5\textwidth}
		\begin{block}{Concetti fondamentali del Bluetooth Low Energy:}
		\begin{itemize}
			\item Banda 2.4 GHz
			\item 40 canali spaziati da 2 MHz
			\item 37 canali per i dati e 3 per gli advertising
			\item FDMA, TDMA
			\item FHSS
		\end{itemize}	
		\end{block}

		\column{0.5\textwidth}
		\begin{block}{Stack protocollare BLE}
			\centering			
			%\includegraphics[width=1\textwidth]{Stack_protocollare_BLE}
		\end{block}
	\end{columns}
	%\begin{block}
		
	%\end{block}
\end{frame}
\begin{frame}
	\frametitle{Sound Event Detection}
	Individuazione di eventi sonori percettivamente simili all'interno di una registrazione.
\end{frame}
\begin{frame}
	\frametitle{Metodo proposto per il few-shot sound event detection}
	\begin{figure}[h]
	\includegraphics[width=.7\textwidth]{Immagini/few_shot_sound_event_detection_method}
	\caption{Metodo proposto per il few-shot sound event detection. (a) Applicazione del modello few-shot, (b) costruzione del set di esempi negativi, in blu, e (c) data augmentation per la generazioni di più esempi positivi, in arancione.}
	\end{figure}
\end{frame}
\begin{frame}
\begin{figure}[h]
	\centering	
	\includegraphics[width=.8\textwidth]{Immagini/few_shot_learning_model}
	\caption{Modello few-shot learning nel caso 5\emph{-way} 2\emph{-shot}}
\end{figure}
\end{frame}
\section{Few-shot learning}

\begin{frame}
	\frametitle{Few-shot learning}
	L'algoritmo di few-shot learning non necessita di un dataset numeroso per il training.
	\begin{block}{Meta-learning}
	L'apprendimento supervisionato tradizionale chiede al modello di riconoscere i dati di training e quindi di generalizzare per dati di test non visti in precedenza. Diversamente, l'obiettivo del meta apprendimento è imparare.
	\end{block}
	 Nel quadro del meta-apprendimento, \textit{impariamo come imparare} a classificare in base a una serie di \textit{episodi di training} e valutiamo utilizzando una serie di episodi di test.
\end{frame}
\begin{frame}
	\frametitle{C-way K-shot}
	Ogni episodio per la classificazione C-way K-shot contiene:
	\begin{itemize}
	\item Un support set costituito da K istanze di C classi.
	\item Un query set con Q istanze per ognuna delle C classi.
	\end{itemize}
	Ogni episodio può essere completamente unico; potremmo non vedere mai le classi di un episodio in nessuno degli altri.
	
	Poiché la rete viene sottoposta ad un compito diverso in ogni fase temporale, deve imparare a discriminare le classi di dati in generale, piuttosto che un particolare sottoinsieme di classi.

\end{frame}
\begin{frame}
	\begin{figure}[h]
		\centering	
		\includegraphics[width=\textwidth]{Immagini/training_task}
		\caption{Esempio episodi}
	\end{figure}
\end{frame}
\section{Creazione del Dataset}

\begin{frame}
	\frametitle{esempio titolo: lettori validi}
\end{frame}

\section{Prototypical}
\begin{frame}
	\frametitle{Protypical Network}
	L'approccio si basa sull'idea che esista un embedding in cui i punti delle istanze di una classe si raggruppano attorno a una singola rappresentazione per ogni classe, chiamata \textit{prototipo}. 
	Nella classificazione few-shot per ogni episodio viene fornito un support set di $N$ esempi etichettati  $S=\{(\mathbf{x}_1,y_1), \dots,(\mathbf{x}_N,y_N)\}$ dove $\mathbf{x}_i\in \mathbb{R}^D$ rappresenta il vettore $D$-dimensionale della feature (nel nostro caso spettrogrammi di dimensione $128 \times 51$) e $y_i \in \{1, \dots, K\}$ la rispettiva label. $S_k$ denota il set di esempi etichettati con la classe $k$.
\end{frame}
\begin{frame}
\frametitle{Relation Network}
\begin{figure}[h]
	\centering
	{\includegraphics[width=0.5\textwidth]{Immagini/proto_few_shot}}
	\caption{I prototipi few-shot $\mathbf{c}_k$ sono calcolati come la media degli embedding del support set per ogni classe. I punti degli embedding delle query sono classificati facendo il softmax sulle distanze del prototipo delle classi: $p_\phi(y = k|\mathbf{x}) \propto \exp \left(-d(f_\phi(\mathbf{x}),\mathbf{c}_k) \right)$.}
\end{figure}
\end{frame}
\begin{frame}
	La rete Prototypical calcola una rappresentazione $M$-dimensionale $\mathbf{c}_k$, o \emph{prototipo}, di ogni classe tramite una funzione di embedding $f_\phi : \mathbb{R}^D \rightarrow \mathbb{R}^M$ con parametri da allenare $\phi$. La funzione di embedding è rappresentata da una rete convoluzionale. Ogni prototipo è calcolato come la media tra gli embedding delle istanze della stessa classe.
	\begin{equation}
		\mathbf{c}_k=\frac{1}{|S_k|}\sum_{(\mathbf{x}_i,y_i)\in S_k}f_\phi (\mathbf{x}_i)
	\end{equation}
	Data una funzione distanza $d: \mathbb{R}^M \times \mathbb{R}^M \rightarrow [0, +\infty )$, la rete Prototypical calcola la relazione di una query $\mathbf{x}$ rispetto ai prototipi tramite la funzione softmax delle distanze prese con segno negativo.
	\begin{equation}\label{eq:softmax}
	p_\phi(y=k|\mathbf{x})=\dfrac{\exp(-d(f_\phi(\mathbf{x}),\mathbf{c}_k))}{\sum_k' \exp(-d(f_\phi(\mathbf{x}),\mathbf{c}_k'))}
	\end{equation}

\end{frame}
\begin{frame}
	Il processo di training avviene minimizzando il negativo del logaritmo della probabilità
	\begin{equation}\label{eq:loss_proto}
		J(\phi)=-\log\left(p_\phi(y=k|\mathbf{x})\right)
	\end{equation}
	considerando la distanza fra query e il prototipo della sua classe.
	Gli episodi di training sono formati campionando C classi di parole da un lettore e selezionando per ognuna casualmente K istanze e Q query.
\end{frame}
\begin{frame}
	\frametitle{Architettura della rete}La CNN che funge da rete di embedding per la Prototypical Network è costituita da 4 strati convoluzionali. Il primo di essi ha come ingresso un singolo canale e come uscita 64 mentre i tre successivi traspongono 64 canali in altri 64.

	Al termine dei blocchi viene effettuato il reshape dell'uscita schiacciandola in una sola dimensione, \texttt{x.view(x.size(0),-1)}, in modo da avere una matrice le cui righe rappresentano gli embedding vettoriali.
\end{frame}

\begin{frame}
	\frametitle{Blocco convoluzionale}
		Ogni blocco convoluzionale ha 4 fasi:
		\begin{itemize}
			\item \texttt{nn.Conv2d(in\_channels,out\_channels,3,padding=1)}: Convoluzione bidimensionale con un kernel 3x3 i cui parametri vengono aggiornati ad ogni backward. Viene effettuato un padding di zeri ai bordi dell'ingresso.
			\item \texttt{nn.BatchNorm2d(out\_channels)}: La batch normalization è un metodo utilizzato per rendere le reti neurali artificiali più veloci e stabili attraverso la normalizzazione degli input dei livelli con re-centering and re-scaling.
			\item \texttt{nn.ReLU()}: il rettificatore è una funzione di attivazione definita come la parte positiva del suo argomento. $f(x)=\max(0,x)$
			\item \texttt{nn.MaxPool2d(2)}: Il max-pooling è un metodo per ridurre la dimensione di un?immagine, suddividendola in blocchi e tenendo solo quello col valore più alto.
		\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Training}
\end{frame}
\begin{frame}
	\frametitle{Validation}
\end{frame}
\begin{frame}
	\frametitle{Test}
\end{frame}

\section{Relation}
\begin{frame}
	\frametitle{Relation Network}
	La rete Relation ha lo scopo di associare due istanze alla volta per determinare la loro similarità.
	Questo viene effettuato concatenando gli embedding di più istanze in un unico elemento che sarà dato in ingresso a una rete decisionale i cui parametri saranno aggiornati in modo che una concatenazione di elementi simili restituisca un risultato vicino a 1.
	La Relation Network è costituita da due moduli: un modulo di \emph{embedding} $f_\varphi$ (equivalente a quello nella Prototypical) e un modulo di \emph{relation} $g_\phi$.
	Le istanze $x_i$ del query set $\mathcal{Q}$ e quelle $x_j$ del support set $\mathcal{S}$ vengono date in ingresso al modulo di embedding producendo dei vettori (feature maps) $f_\varphi(x_i)$ e $f_\varphi(x_j)$.
	Questi ultimi vengono poi dati all'operatore $\mathcal{C}(\cdot ,\cdot)$ che ne fa la concatenazione: $\mathcal{C}(f_\varphi(x_i),f_\varphi(x_j))$.
\end{frame}
\begin{frame}
	Le feature map concatenate passano poi attraverso il modulo di decisione che restituisce uno scalare da 0 a 1, il quale rappresenta la somiglianza tra $x_i$ e $x_j$.
	Per il caso $C$-way one-shot, viene concatenata la query con le istanze delle $C$ classi producendo $C$ punteggi di somiglianza.
	\begin{equation}
		r_{i,j}=g_\phi(\mathcal{C}(f_\varphi(x_i),f_\varphi(x_j))),  \qquad i = 1, 2, \dots, C
	\end{equation}
	Nel caso $C$-way K-shot invece, la query viene concatenata con la somma elemento per elemento degli embedding di ogni istanza delle classi. Quindi, in entrambi i casi i confronti $r_{i,j}$ sono $C$ per ogni query.
	
	Per allenare il modello viene usato l'errore quadratico medio (MSE) in modo che l'uscita del modulo di decisione produca 1 se i vettori concatenati sono della stessa classe e 0 altrimenti.
\end{frame}
\begin{frame}
\frametitle{Relation Network}
\begin{figure}[h]
	\centering	
	\includegraphics[width=\textwidth]{Immagini/relation_network}
	\caption{Architettura della Relation Network nel caso 5\emph{-way} 1\emph{-shot} con un esempio di query.}
\end{figure}
\end{frame}
\begin{frame}
	\frametitle{Training}
\end{frame}
\begin{frame}
	\frametitle{Validation}
\end{frame}
\begin{frame}
	\frametitle{Test}
\end{frame}

\section{Risultati}
\begin{frame}
	\frametitle{Matrice di confusione}
	Nella fase di test, la rete elabora una predizione dell'output a partire da un ingresso noto che poi viene confrontata con il valore effettivo.
	Nel nostro caso è presente un positive set e un negative set, si tratta quindi di classificazione binaria.
	Il confronto tra predizione e label può produrre quattro risultati:
	\begin{itemize}
		\item Vero Negativo (TN): il valore reale è negativo e il valore predetto è negativo;
		\item Vero Positivo (TP): il valore reale è positivo e il valore predetto	è positivo;
		\item Falso Negativo (FN): il valore reale è positivo e il valore predetto è negativo;
		\item Falso Positivo (FP): il valore reale è negativo e il valore predetto	è positivo;
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Precision e Recall}
	\begin{block}{Precision}
	Il parametro Precision rappresenta quanti tra i casi predetti come positivi sono realmente positivi.
	\end{block}
	\begin{block}{Recall}
	Il parametro Recall indica quanti tra i casi realmente positivi è stato predetto in modo corretto.
	\end{block}
	\begin{equation}
	Precision=\frac{TP}{TP+FP}
	\end{equation}
	\begin{equation}
	Recall=\frac{TP}{TP+FN}
	\end{equation}
\end{frame}
\begin{frame}
	\frametitle{AUPRC}
	\begin{block}{Area under precision-recall curve (AUPRC)}
	Area sottessa dalla curva precision-recall.
	Questo valore è molto utile quando i dati sono sbilanciati e, in una classificazione binaria, siamo più interessati al riconoscimento di una classe in particolare.
	\end{block}
	Per calcolare questo parametro sono state usate le funzioni \texttt{precision\_recall\_curve} e \texttt{sklearn.metrics.auc}.
\end{frame}
\begin{frame}
	\frametitle{Calcolo AUPRC}
	\begin{enumerate}
	\item Consideriamo i valori di probabilità delle query di appartenere alla classe positiva.
	\item Ordiniamo il vettore in ordine decrescente.
	\item Consideriamo come soglia il valore di probabilità presente al primo elemento e calcoliamo precision e recall confrontando con un vettore che indica la classe corretta.
	\item Spostiamo la soglia al valore del secondo elemento e calcoliamo un'altra coppia di parametri.
	\item Ripetiamo il passo 3 e 4 per tutti gli elementi.
	\item Otteniamo una curva considerando alle ascisse i valori di recall e come ordinate i valori di precision.
	\item Calcolando l'area sottesa dalla curva ricaviamo il parametro AUPRC.
	\end{enumerate}

\end{frame}
\begin{frame}
	\frametitle{Risultati}
\end{frame}
\section{Conclusioni}
\begin{frame}
	\frametitle{Conclusioni}
\end{frame}


\end{document}