\documentclass[11pt]{beamer}
\title[Digital Adaptive Circuits for Learning Systems]{Sound Event Detection con la tecnica del ``few-shot learning''}
\author{Matteo Orlandini e Jacopo Pagliuca}
\date{\today}
\institute[UnivPM]{Università Politecnica delle Marche}
%\logo{\includegraphics[width=15mm]{Immagini/univpmlogo}}
\titlegraphic{
\includegraphics[width=2cm]{Immagini/univpmlogo}
}
\usepackage[english, italian]{babel} %l?ultima lingua dichiarata è la lingua principale del documento
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{animate} %per le gif 
\usepackage{xcolor}
\usepackage{listings}

\lstset{% general command to set parameter(s)
	backgroundcolor=\color{white},
	%backgroundcolor=\color{White}, % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
	basicstyle=\small,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,
	captionpos=b,                    % sets the caption-position to bottom
	%extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	keywordstyle=\color{blue}\bfseries, 	% blue keywords
	language=C,
	classoffset=0,
	morekeywords={bool,ifdef,ifndef,endif},
	keywordstyle=\color{blue}\bfseries,
	classoffset=1,
	morekeywords={__LOG_INIT,__LOG,adv_start,bearer_adtype_add,advertiser_enable,advertiser_packet_alloc,memcpy,advertiser_packet_send,ERROR_CHECK,mesh_provisionee_prov_start,adv_start,mesh_app_uuid_print,nrf_mesh_configure_device_uuid_get,mesh_stack_start,nrf_mesh_enable,provisioning_complete_cb,dsm_local_unicast_addresses_get,hal_led_blink_stop,hal_led_mask_set,hal_led_mask_set,device_identification_start_cb,hal_led_blink_ms,LED_BLINK_ATTENTION_COUNT,provisioning_aborted_cb,hal_led_blink_stop,nrf_mesh_rx_cb_set,adv_init,LEDS_OFF,sprintf,__LOG_XB,LEDS_ON,advertiser_instance_init,power_management_init,nrf_pwr_mgmt_init,APP_ERROR_CHECK,MSEC_TO_UNITS,memset,ble_advdata_encode,sd_ble_gap_adv_set_configure,sd_ble_gap_adv_start,bsp_indication_set,gap_params_init,conn_params_init,nrf_sdh_enable_request,nrf_sdh_ble_default_cfg_set,nrf_sdh_ble_enable,ble_stack_init,advertising_init,advertising_start,initialize,start,gatt_init,nrf_ble_gatt_init,NRF_BLE_GATT_DEF,hal_led_pin_set,nrf_delay_ms,NRF_MESH_ERROR_CHECK,sd_ble_gatts_characteristic_add,sd_ble_gatts_characteristic_add,BLE_GAP_CONN_SEC_MODE_SET_OPEN,advertiser_packet_discard,defined,on_es_evt,bsp_board_led_invert,bsp_board_led_on,bsp_board_led_off,nrf_ble_es_init,nrf_mesh_on_sd_evt, nrf_ble_es_on_start_connectable_advertising},
	keywordstyle=\color{black}\bfseries,
	classoffset=0,
	identifierstyle=,           % nothing happens
	commentstyle=\color{green}, % green comments
	stringstyle=\color{red},  %\ttfamily    % typewriter type for strings
	showstringspaces=false,	% no special string spaces
	numberbychapter=true,
	tabsize=2,
	columns=flexile,
	keepspaces=false,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	frame=lines,
	literate=
	{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
	{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
	{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
	{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
	{ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
	{Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
	{â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
	{Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
	{Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
	{?}{{\oe}}1 {?}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
	{?}{{\H{u}}}1 {?}{{\H{U}}}1 {?}{{\H{o}}}1 {?}{{\H{O}}}1
	{ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
	{?}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
	{»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}  

\definecolor{maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{halfgray}{gray}{0.55}
\definecolor{ipython_frame}{RGB}{207, 207, 207}
\definecolor{ipython_bg}{RGB}{247, 247, 247}
\definecolor{ipython_red}{RGB}{186, 33, 33}
\definecolor{ipython_green}{RGB}{0, 128, 0}
\definecolor{ipython_cyan}{RGB}{64, 128, 128}
\definecolor{ipython_purple}{RGB}{170, 34, 255}


\lstdefinelanguage{iPython}{
	morekeywords={access,and,break,class,continue,def,del,elif,else,except,exec,finally,for,from,global,if,import,in,is,lambda,not,or,pass,print,raise,return,try,while},%
	%
	% Built-ins
	morekeywords=[2]{abs,all,any,basestring,bin,bool,bytearray,callable,chr,classmethod,cmp,compile,complex,delattr,dict,dir,divmod,enumerate,eval,execfile,file,filter,float,format,frozenset,getattr,globals,hasattr,hash,help,hex,id,input,int,isinstance,issubclass,iter,len,list,locals,long,map,memoryview,next,object,oct,open,ord,pow,property,range,raw_input,reduce,reload,repr,reversed,round,set,setattr,slice,sorted,staticmethod,str,sum,super,tuple,type,unichr,unicode,vars,xrange,zip,apply,buffer,coerce,intern, F, nn, np},%
	%
	sensitive=true,%
	morecomment=[l]\#,%
	morestring=[b]',%
	morestring=[b]",%
	%
	morestring=[s]{'''}{'''},% used for documentation text (mulitiline strings)
	morestring=[s]{"""}{"""},% added by Philipp Matthias Hahn
	%
	morestring=[s]{r'}{'},% `raw' strings
	morestring=[s]{r"}{"},%
	morestring=[s]{r'''}{'''},%
	morestring=[s]{r"""}{"""},%
	morestring=[s]{u'}{'},% unicode strings
	morestring=[s]{u"}{"},%
	morestring=[s]{u'''}{'''},%
	morestring=[s]{u"""}{"""},%
	%
	% {replace}{replacement}{lenght of replace}
	% *{-}{-}{1} will not replace in comments and so on
	literate=
	{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
	{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
	{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
	{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
	{ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
	{Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
	{â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
	{Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
	{?}{{\oe}}1 {?}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
	{ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
	{?}{{\EUR}}1 {£}{{\pounds}}1
	%
	{^}{{{\color{ipython_purple}\^{}}}}1
	{=}{{{\color{ipython_purple}=}}}1
	%
	{+}{{{\color{ipython_purple}+}}}1
	{*}{{{\color{ipython_purple}$^\ast$}}}1
	{/}{{{\color{ipython_purple}/}}}1
	%
	{+=}{{{+=}}}1
	{-=}{{{-=}}}1
	{*=}{{{$^\ast$=}}}1
	{/=}{{{/=}}}1,
	literate=
	*{-}{{{\color{ipython_purple}-}}}1
	{?}{{{\color{ipython_purple}?}}}1,
	%
	identifierstyle=\color{black}\ttfamily\tiny,
	commentstyle=\color{ipython_cyan}\ttfamily\tiny,
	stringstyle=\color{ipython_red}\ttfamily\tiny,
	keepspaces=false,
	showspaces=false,
	breaklines=true,
	showstringspaces=false,
	%
	rulecolor=\color{ipython_frame},
	frame=single,
	frameround={t}{t}{t}{t},
	framexleftmargin=6mm,
	numbers=left,
	numberstyle=\tiny\color{halfgray},
	%
	%
	backgroundcolor=\color{ipython_bg},
	%   extendedchars=true,
	basicstyle=\scriptsize\tiny,
	tabsize=1,
	keywordstyle=\color{ipython_green}\ttfamily\tiny,
	morekeywords={typeof, null, catch, switch, in, int, str, float, self, import, def, return, True, False, None},
	emph={read_json_file,write_json_file, exists, parse, getroot, basename, scandir, keys, Counter, append, sample, isdir, compute_melspectrogram, load, melspectrogram,power_to_db, mkdir, save_dataset, save_test_dataset, OSError, find_valid_readers, create_training_validation_test_readers, find_classes, concatenate, FloatTensor, save, load, tqdm, empty, count_parameters, conv_block, Protonet, Module, init,Sequential, forward, is_available, Adam, get_training_validation_readers, trange, batch_sample, to, train, zero_grad, loss, backward, step, batch_sample, mean, savemat,get_training_validation_readers, empty, shape, cat, unsqueeze, expand, squeeze, size, arange, view, expand, mean, euclidean_dist, log_softmax, gather, eq, max, min, Adam, load_state_dict, isfile, listdir, get_negative_positive_query_set, test_predictions, cpu, tolist, precision_recall_curve, auc, std, get_negative_positive_query_set, test_predictions, size, softmax, detach, reshape, squeeze, Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, Linear, normal_,sigmoid,relu,zero_,fill_,sqrt,get_device_properties, interpolate, repeat, unsqueeze, transpose, weights_init, init, ones, zeros,main, extend, clip_grad_norm_},          % Custom highlighting
	emphstyle=\color{ipython_purple}\ttfamily,    % Custom highlighting style
}


\usetheme{Antibes}
\usecolortheme{default} %https://hartwork.org/beamer-theme-matrix/
%\useoutertheme[right,color=red]{sidebar} %{infolines}
%\setbeamertemplate{sidebar canvas right}[vertical shading][top=red,bottom=white]
\setbeamercovered{dynamic}

\begin{document}
	\begin{frame}
		\maketitle
	\end{frame}

\begin{frame}
	\frametitle{Contenuti}
	\tableofcontents
\end{frame}

\section{Introduzione}
\begin{frame}
	\frametitle{Bluetooth Low Energy}
	\begin{columns}
		\column{0.5\textwidth}
		\begin{block}{Concetti fondamentali del Bluetooth Low Energy:}
		\begin{itemize}
			\item Banda 2.4 GHz
			\item 40 canali spaziati da 2 MHz
			\item 37 canali per i dati e 3 per gli advertising
			\item FDMA, TDMA
			\item FHSS
		\end{itemize}	
		\end{block}

		\column{0.5\textwidth}
		\begin{block}{Stack protocollare BLE}
			\centering			
			%\includegraphics[width=1\textwidth]{Stack_protocollare_BLE}
		\end{block}
	\end{columns}
	%\begin{block}
		
	%\end{block}
\end{frame}
\begin{frame}
	\frametitle{Sound Event Detection}
	Individuazione di eventi sonori percettivamente simili all'interno di una registrazione.
\end{frame}
\begin{frame}
	\frametitle{Metodo proposto per il few-shot sound event detection}
	\begin{figure}[h]
	\includegraphics[width=.7\textwidth]{Immagini/few_shot_sound_event_detection_method}
	\caption{Metodo proposto per il few-shot sound event detection. (a) Applicazione del modello few-shot, (b) costruzione del set di esempi negativi, in blu, e (c) data augmentation per la generazioni di più esempi positivi, in arancione.}
	\end{figure}
\end{frame}
\begin{frame}
\begin{figure}[h]
	\centering	
	\includegraphics[width=.8\textwidth]{Immagini/few_shot_learning_model}
	\caption{Modello few-shot learning nel caso 5\emph{-way} 2\emph{-shot}}
\end{figure}
\end{frame}
\section{Few-shot learning}

\begin{frame}
	\frametitle{Few-shot learning}
	L'algoritmo di few-shot learning non necessita di un dataset numeroso per il training.
	\begin{block}{Meta-learning}
	L'apprendimento supervisionato tradizionale chiede al modello di riconoscere i dati di training e quindi di generalizzare per dati di test non visti in precedenza. Diversamente, l'obiettivo del meta apprendimento è imparare.
	\end{block}
	 Nel quadro del meta-apprendimento, \textit{impariamo come imparare} a classificare in base a una serie di \textit{episodi di training} e valutiamo utilizzando una serie di episodi di test.
\end{frame}
\begin{frame}
	\frametitle{C-way K-shot}
	Ogni episodio per la classificazione C-way K-shot contiene:
	\begin{itemize}
	\item Un support set costituito da K istanze di C classi.
	\item Un query set con Q istanze per ognuna delle C classi.
	\end{itemize}
	Ogni episodio può essere completamente unico; potremmo non vedere mai le classi di un episodio in nessuno degli altri.
	
	Poiché la rete viene sottoposta ad un compito diverso in ogni fase temporale, deve imparare a discriminare le classi di dati in generale, piuttosto che un particolare sottoinsieme di classi.

\end{frame}
\begin{frame}
	\begin{figure}[h]
		\centering	
		\includegraphics[width=\textwidth]{Immagini/training_task}
		\caption{Esempio episodi}
	\end{figure}
\end{frame}
\section{Creazione del Dataset}

\begin{frame}
	\frametitle{esempio titolo: lettori validi}
\end{frame}

\section{Prototypical}
\begin{frame}
	\frametitle{Protypical Network}
	L'approccio si basa sull'idea che esista un embedding in cui i punti delle istanze di una classe si raggruppano attorno a una singola rappresentazione per ogni classe, chiamata \textit{prototipo}. 
	Nella classificazione few-shot per ogni episodio viene fornito un support set di $N$ esempi etichettati  $S=\{(\mathbf{x}_1,y_1), \dots,(\mathbf{x}_N,y_N)\}$ dove $\mathbf{x}_i\in \mathbb{R}^D$ rappresenta il vettore $D$-dimensionale della feature (nel nostro caso spettrogrammi di dimensione $128 \times 51$) e $y_i \in \{1, \dots, K\}$ la rispettiva label. $S_k$ denota il set di esempi etichettati con la classe $k$.
\end{frame}
\begin{frame}
\begin{figure}[h]
	\centering
	{\includegraphics[width=0.5\textwidth]{Immagini/proto_few_shot}}
	\caption{I prototipi few-shot $\mathbf{c}_k$ sono calcolati come la media degli embedding del support set per ogni classe. I punti degli embedding delle query sono classificati facendo il softmax sulle distanze del prototipo delle classi: $p_\phi(y = k|\mathbf{x}) \propto \exp \left(-d(f_\phi(\mathbf{x}),\mathbf{c}_k) \right)$.}
\end{figure}
\end{frame}
\begin{frame}
	La rete Prototypical calcola una rappresentazione $M$-dimensionale $\mathbf{c}_k$, o \emph{prototipo}, di ogni classe tramite una funzione di embedding $f_\phi : \mathbb{R}^D \rightarrow \mathbb{R}^M$ con parametri da allenare $\phi$. La funzione di embedding è rappresentata da una rete convoluzionale. Ogni prototipo è calcolato come la media tra gli embedding delle istanze della stessa classe.
	\begin{equation}
		\mathbf{c}_k=\frac{1}{|S_k|}\sum_{(\mathbf{x}_i,y_i)\in S_k}f_\phi (\mathbf{x}_i)
	\end{equation}
	Data una funzione distanza $d: \mathbb{R}^M \times \mathbb{R}^M \rightarrow [0, +\infty )$, la rete Prototypical calcola la relazione di una query $\mathbf{x}$ rispetto ai prototipi tramite la funzione softmax delle distanze prese con segno negativo.
	\begin{equation}\label{eq:softmax}
	p_\phi(y=k|\mathbf{x})=\dfrac{\exp(-d(f_\phi(\mathbf{x}),\mathbf{c}_k))}{\sum_k' \exp(-d(f_\phi(\mathbf{x}),\mathbf{c}_k'))}
	\end{equation}

\end{frame}
\begin{frame}
	Il processo di training avviene minimizzando il negativo del logaritmo della probabilità
	\begin{equation}\label{eq:loss_proto}
		J(\phi)=-\log\left(p_\phi(y=k|\mathbf{x})\right)
	\end{equation}
	considerando la distanza fra query e il prototipo della sua classe.
	Gli episodi di training sono formati campionando C classi di parole da un lettore e selezionando per ognuna casualmente K istanze e Q query.
\end{frame}
\begin{frame}
	\frametitle{Architettura della rete}La CNN che funge da rete di embedding per la Prototypical Network è costituita da 4 strati convoluzionali. Il primo di essi ha come ingresso un singolo canale e come uscita 64 mentre i tre successivi traspongono 64 canali in altri 64.

	Al termine dei blocchi viene effettuato il reshape dell'uscita schiacciandola in una sola dimensione, \texttt{x.view(x.size(0),-1)}, in modo da avere una matrice le cui righe rappresentano gli embedding vettoriali.
\end{frame}
\begin{frame}[fragile]
\begin{lstlisting}[language=iPython,firstnumber=1, caption=protonet.py, label= Protonet,captionpos=b]
import torch.nn as nn

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def conv_block(in_channels,out_channels):

    return nn.Sequential(
        nn.Conv2d(in_channels,out_channels,3,padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(),
        nn.MaxPool2d(2)
    )

class Protonet(nn.Module):
    def __init__(self):
        super(Protonet,self).__init__()
        self.encoder = nn.Sequential(
            conv_block(1,64),
            conv_block(64,64),
            conv_block(64,64),
            conv_block(64,64)
        )
    def forward(self,x):
        (num_samples,mel_bins, seq_len) = x.shape
        x = x.view(-1,1,mel_bins,seq_len) 
        x = self.encoder(x)
        return x.view(x.size(0),-1)
\end{lstlisting}
\end{frame}
\begin{frame}
	\frametitle{Blocco convoluzionale}
		Ogni blocco convoluzionale ha 4 fasi:
		\begin{itemize}
			\item \texttt{nn.Conv2d(in\_channels,out\_channels,3,padding=1)}: Convoluzione bidimensionale con un kernel 3x3 i cui parametri vengono aggiornati ad ogni backward. Viene effettuato un padding di zeri ai bordi dell'ingresso.
			\item \texttt{nn.BatchNorm2d(out\_channels)}: La batch normalization è un metodo utilizzato per rendere le reti neurali artificiali più veloci e stabili attraverso la normalizzazione degli input dei livelli con re-centering and re-scaling.
			\item \texttt{nn.ReLU()}: il rettificatore è una funzione di attivazione definita come la parte positiva del suo argomento. $f(x)=\max(0,x)$
			\item \texttt{nn.MaxPool2d(2)}: Il max-pooling è un metodo per ridurre la dimensione di un'immagine, suddividendola in blocchi e tenendo solo quello col valore più alto.
		\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Training}
	Il numero totale di iterazioni è 60000; per ogni episodio viene richiamata la funzione \texttt{batch\_sample}.
	
	Con \texttt{loss\_out.backward()} viene utilizzata la loss per la retropropagazione che aggiorna i parametri della rete.
\end{frame}
\begin{frame}
	La funzione \texttt{get\_training\_validation\_readers}, a partire dalla lista di lettori di training/validation, seleziona quelli che hanno almeno un numero di parole diverse pari a C e li suddivide tra training e validation seguendo il rapporto usato nel paper di riferimento.
\end{frame}
\begin{frame}
	La funzione \texttt{batch\_sample} campiona a caso uno dei lettori di training e di questo seleziona casualmente $C$ parole. Per ognuna di queste parole si ricava il numero di istanze presenti del dataset e vengono campionate $K+Q$ istanze random della parola. Le $K+Q$ istanze vengono divise in $K$ istanze del support set e $Q$ stanze del query set. La funzione ritorna due tensori \texttt{query} e \texttt{support}, rispettivamente di dimensioni $C \times Q \times 128 \times 51$ e $C \times K \times 128 \times 51$.
\end{frame}
\begin{frame}
	Una volta ottenute, le feature del support set e query set vengono date in ingresso alla funzione \texttt{loss}. Entrambi i set vengono ridimensionati, rispettivamente da $C \times K \times 128 \times 51$ a $(C \cdot K) \times 128 \times 51$ e da $C \times Q \times 128 \times 51$ a $(C \cdot Q) \times 128 \times 51$.
	Vengono poi concatenate, risultando in un tensore $\left(C \cdot (Q + K)\right) \times 128 \times 51$, per poter essere passate al modello che calcolerà gli embedding per ogni istanza.
	Gli elementi del support set che fanno parte della stessa classe andranno a costituire i prototipi tramite una media dei loro valori.
\end{frame}
\begin{frame}
	Vengono successivamente calcolate le distanze rispetto ai prototipi degli embedding di ogni classe tramite la funzione \texttt{euclidean\_dist} che ritorna una matrice di dimensioni $(C \cdot Q) \times C$. L'i-esima riga e la j-esima colonna rappresentano la distanza euclidea tra gli embedding della i-esima query ($i = 1, \dots, C \cdot Q$) e quelli del j-esimo ($j = 1, \dots, C$) prototipo.
	
	Per ogni query viene poi usata la funzione di attivazione softmax, i cui argomenti sono le distanze tra la query e i prototipi delle varie classi. La funzione loss da minimizzare è il logaritmo del softmax preso con segno negativo. Se la query è vicina al prototipo, la funzione softmax tende a 1 e di conseguenza il suo logaritmo a 0, dunque la loss tende a 0, come dovrebbe appunto risultare. Nel caso in cui, invece, la query è molto lontata dal prototipo, la funzione softmax tende a 0 e il suo logaritmo a $-\infty$. In questo caso, la loss, definita come il logaritmo del softmax preso con segno negativo, tende a $+\infty$, come ci si aspetta.
\end{frame}
\begin{frame}
	\frametitle{Validation}
	Ogni 5000 episodi del training vengono effettuati 1000 episodi di validation.
	Come nel training vengono ricavati support set e query set, ma in questo caso dai lettori di validation.
	Per questi vengono calcolati con la funzione \texttt{loss} allo stesso modo loss e accuracy e vengono salvati.
	In questo caso però i parametri del modello non vengono aggiornati.
	Questo processo è necessario per verificare l'effettività del modello su ingressi mai visti in fase di training.
	Al termine dei 1000 episodi di validation, se la media dell'accuracy è migliore di quella calcolata nel passo precedente il modello viene salvato e sovrascritto, altrimenti viene ignorato. In questo modo si salva il modello migliore durante il training, cercando di evitare sia underfitting sia overfitting.
\end{frame}
\begin{frame}
	\frametitle{Test}
\end{frame}

\section{Relation}
\begin{frame}
	\frametitle{Relation Network}
	La rete Relation ha lo scopo di associare due istanze alla volta per determinare la loro similarità.
	Questo viene effettuato concatenando gli embedding di più istanze in un unico elemento che sarà dato in ingresso a una rete decisionale i cui parametri saranno aggiornati in modo che una concatenazione di elementi simili restituisca un risultato vicino a 1.
	La Relation Network è costituita da due moduli: un modulo di \emph{embedding} $f_\varphi$ (equivalente a quello nella Prototypical) e un modulo di \emph{relation} $g_\phi$.
	Le istanze $x_i$ del query set $\mathcal{Q}$ e quelle $x_j$ del support set $\mathcal{S}$ vengono date in ingresso al modulo di embedding producendo dei vettori (feature maps) $f_\varphi(x_i)$ e $f_\varphi(x_j)$.
	Questi ultimi vengono poi dati all'operatore $\mathcal{C}(\cdot ,\cdot)$ che ne fa la concatenazione: $\mathcal{C}(f_\varphi(x_i),f_\varphi(x_j))$.
\end{frame}
\begin{frame}
	Le feature map concatenate passano poi attraverso il modulo di decisione che restituisce uno scalare da 0 a 1, il quale rappresenta la somiglianza tra $x_i$ e $x_j$.
	Per il caso $C$-way one-shot, viene concatenata la query con le istanze delle $C$ classi producendo $C$ punteggi di somiglianza.
	\begin{equation}
		r_{i,j}=g_\phi(\mathcal{C}(f_\varphi(x_i),f_\varphi(x_j))),  \qquad i = 1, 2, \dots, C
	\end{equation}
	Nel caso $C$-way K-shot invece, la query viene concatenata con la somma elemento per elemento degli embedding di ogni istanza delle classi. Quindi, in entrambi i casi i confronti $r_{i,j}$ sono $C$ per ogni query.
	
	Per allenare il modello viene usato l'errore quadratico medio (MSE) in modo che l'uscita del modulo di decisione produca 1 se i vettori concatenati sono della stessa classe e 0 altrimenti.
\end{frame}
\begin{frame}
\frametitle{Relation Network}
\begin{figure}[h]
	\centering	
	\includegraphics[width=\textwidth]{Immagini/relation_network}
	\caption{Architettura della Relation Network nel caso 5\emph{-way} 1\emph{-shot} con un esempio di query.}
\end{figure}
\end{frame}
\begin{frame}
	\frametitle{Training}
\end{frame}
\begin{frame}
	\frametitle{Validation}
\end{frame}
\begin{frame}
	\frametitle{Test}
\end{frame}

\section{Risultati}
\begin{frame}
	\frametitle{Matrice di confusione}
	Nella fase di test, la rete elabora una predizione dell'output a partire da un ingresso noto che poi viene confrontata con il valore effettivo.
	Nel nostro caso è presente un positive set e un negative set, si tratta quindi di classificazione binaria.
	Il confronto tra predizione e label può produrre quattro risultati:
	\begin{itemize}
		\item Vero Negativo (TN): il valore reale è negativo e il valore predetto è negativo;
		\item Vero Positivo (TP): il valore reale è positivo e il valore predetto	è positivo;
		\item Falso Negativo (FN): il valore reale è positivo e il valore predetto è negativo;
		\item Falso Positivo (FP): il valore reale è negativo e il valore predetto	è positivo;
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Precision e Recall}
	\begin{block}{Precision}
	Il parametro Precision rappresenta quanti tra i casi predetti come positivi sono realmente positivi.
	\end{block}
	\begin{block}{Recall}
	Il parametro Recall indica quanti tra i casi realmente positivi è stato predetto in modo corretto.
	\end{block}
	\begin{equation}
	Precision=\frac{TP}{TP+FP}
	\end{equation}
	\begin{equation}
	Recall=\frac{TP}{TP+FN}
	\end{equation}
\end{frame}
\begin{frame}
	\frametitle{AUPRC}
	\begin{block}{Area under precision-recall curve (AUPRC)}
	Area sottessa dalla curva precision-recall.
	Questo valore è molto utile quando i dati sono sbilanciati e, in una classificazione binaria, siamo più interessati al riconoscimento di una classe in particolare.
	\end{block}
	Per calcolare questo parametro sono state usate le funzioni \texttt{precision\_recall\_curve} e \texttt{sklearn.metrics.auc}.
\end{frame}
\begin{frame}
	\frametitle{Calcolo AUPRC}
	\begin{enumerate}
	\item Consideriamo i valori di probabilità delle query di appartenere alla classe positiva.
	\item Ordiniamo il vettore in ordine decrescente.
	\item Consideriamo come soglia il valore di probabilità presente al primo elemento e calcoliamo precision e recall confrontando con un vettore che indica la classe corretta.
	\item Spostiamo la soglia al valore del secondo elemento e calcoliamo un'altra coppia di parametri.
	\item Ripetiamo il passo 3 e 4 per tutti gli elementi.
	\item Otteniamo una curva considerando alle ascisse i valori di recall e come ordinate i valori di precision.
	\item Calcolando l'area sottesa dalla curva ricaviamo il parametro AUPRC.
	\end{enumerate}

\end{frame}
\begin{frame}
	\frametitle{Risultati}
\end{frame}
\section{Conclusioni}
\begin{frame}
	\frametitle{Conclusioni}
\end{frame}


\end{document}