\documentclass[11pt]{beamer}
\title[Digital Adaptive Circuits for Learning Systems]{Sound Event Detection con la tecnica del ``few-shot learning''}
\author{Matteo Orlandini e Jacopo Pagliuca}
\date{\today}
\institute[UnivPM]{Università Politecnica delle Marche}
%\logo{\includegraphics[width=15mm]{Immagini/univpmlogo}}
\titlegraphic{
\includegraphics[width=2cm]{Immagini/univpmlogo}
}
\usepackage[english, italian]{babel} %l?ultima lingua dichiarata è la lingua principale del documento
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{subfig}
\usepackage{graphicx}
\usepackage{animate} %per le gif 
\usepackage{xcolor}
\usepackage{listings}
\usepackage{booktabs} %toprule, midrule, bottomrule
\usepackage{subcaption,booktabs}

\lstset{% general command to set parameter(s)
	backgroundcolor=\color{white},
	%backgroundcolor=\color{White}, % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
	basicstyle=\small,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,
	captionpos=b,                    % sets the caption-position to bottom
	%extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	keywordstyle=\color{blue}\bfseries, 	% blue keywords
	language=C,
	classoffset=0,
	morekeywords={bool,ifdef,ifndef,endif},
	keywordstyle=\color{blue}\bfseries,
	keywordstyle=\color{black}\bfseries,
	classoffset=0,
	identifierstyle=,           % nothing happens
	commentstyle=\color{green}, % green comments
	stringstyle=\color{red},  %\ttfamily    % typewriter type for strings
	showstringspaces=false,	% no special string spaces
	numberbychapter=true,
	tabsize=2,
	columns=flexile,
	keepspaces=false,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	frame=lines,
	literate=
	{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
	{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
	{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
	{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
	{ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
	{Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
	{â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
	{Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
	{Ã}{{\~A}}1 {ã}{{\~a}}1 {Õ}{{\~O}}1 {õ}{{\~o}}1
	{?}{{\oe}}1 {?}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
	{?}{{\H{u}}}1 {?}{{\H{U}}}1 {?}{{\H{o}}}1 {?}{{\H{O}}}1
	{ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
	{?}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
	{»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}  

\definecolor{maroon}{cmyk}{0, 0.87, 0.68, 0.32}
\definecolor{halfgray}{gray}{0.55}
\definecolor{ipython_frame}{RGB}{207, 207, 207}
\definecolor{ipython_bg}{RGB}{247, 247, 247}
\definecolor{ipython_red}{RGB}{186, 33, 33}
\definecolor{ipython_green}{RGB}{0, 128, 0}
\definecolor{ipython_cyan}{RGB}{64, 128, 128}
\definecolor{ipython_purple}{RGB}{170, 34, 255}


\lstdefinelanguage{iPython}{
	morekeywords={access,and,break,class,continue,def,del,elif,else,except,exec,finally,for,from,global,if,import,in,is,lambda,not,or,pass,print,raise,return,try,while},%
	%
	% Built-ins
	morekeywords=[2]{abs,all,any,basestring,bin,bool,bytearray,callable,chr,classmethod,cmp,compile,complex,delattr,dict,dir,divmod,enumerate,eval,execfile,file,filter,float,format,frozenset,getattr,globals,hasattr,hash,help,hex,id,input,int,isinstance,issubclass,iter,len,list,locals,long,map,memoryview,next,object,oct,open,ord,pow,property,range,raw_input,reduce,reload,repr,reversed,round,set,setattr,slice,sorted,staticmethod,str,sum,super,tuple,type,unichr,unicode,vars,xrange,zip,apply,buffer,coerce,intern, F, nn, np},%
	%
	sensitive=true,%
	morecomment=[l]\#,%
	morestring=[b]',%
	morestring=[b]",%
	%
	morestring=[s]{'''}{'''},% used for documentation text (mulitiline strings)
	morestring=[s]{"""}{"""},% added by Philipp Matthias Hahn
	%
	morestring=[s]{r'}{'},% `raw' strings
	morestring=[s]{r"}{"},%
	morestring=[s]{r'''}{'''},%
	morestring=[s]{r"""}{"""},%
	morestring=[s]{u'}{'},% unicode strings
	morestring=[s]{u"}{"},%
	morestring=[s]{u'''}{'''},%
	morestring=[s]{u"""}{"""},%
	%
	% {replace}{replacement}{lenght of replace}
	% *{-}{-}{1} will not replace in comments and so on
	literate=
	{á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
	{Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
	{à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
	{À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
	{ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
	{Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
	{â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
	{Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
	{?}{{\oe}}1 {?}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
	{ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
	{?}{{\EUR}}1 {£}{{\pounds}}1
	%
	{^}{{{\color{ipython_purple}\^{}}}}1
	{=}{{{\color{ipython_purple}=}}}1
	%
	{+}{{{\color{ipython_purple}+}}}1
	{*}{{{\color{ipython_purple}$^\ast$}}}1
	{/}{{{\color{ipython_purple}/}}}1
	%
	{+=}{{{+=}}}1
	{-=}{{{-=}}}1
	{*=}{{{$^\ast$=}}}1
	{/=}{{{/=}}}1,
	literate=
	*{-}{{{\color{ipython_purple}-}}}1
	{?}{{{\color{ipython_purple}?}}}1,
	%
	identifierstyle=\color{black}\ttfamily\tiny,
	commentstyle=\color{ipython_cyan}\ttfamily\tiny,
	stringstyle=\color{ipython_red}\ttfamily\tiny,
	keepspaces=false,
	showspaces=false,
	breaklines=true,
	showstringspaces=false,
	%
	rulecolor=\color{ipython_frame},
	frame=single,
	frameround={t}{t}{t}{t},
	framexleftmargin=6mm,
	numbers=left,
	numberstyle=\tiny\color{halfgray},
	%
	%
	backgroundcolor=\color{ipython_bg},
	%   extendedchars=true,
	basicstyle=\scriptsize\tiny,
	tabsize=1,
	keywordstyle=\color{ipython_green}\ttfamily\tiny,
	morekeywords={typeof, null, catch, switch, in, int, str, float, self, import, def, return, True, False, None},
	emph={read_json_file,write_json_file, exists, parse, getroot, basename, scandir, keys, Counter, append, sample, isdir, compute_melspectrogram, load, melspectrogram,power_to_db, mkdir, save_dataset, save_test_dataset, OSError, find_valid_readers, create_training_validation_test_readers, find_classes, concatenate, FloatTensor, save, load, tqdm, empty, count_parameters, conv_block, Protonet, Module, init,Sequential, forward, is_available, Adam, get_training_validation_readers, trange, batch_sample, to, train, zero_grad, loss, backward, step, batch_sample, mean, savemat,get_training_validation_readers, empty, shape, cat, unsqueeze, expand, squeeze, size, arange, view, expand, mean, euclidean_dist, log_softmax, gather, eq, max, min, Adam, load_state_dict, isfile, listdir, get_negative_positive_query_set, test_predictions, cpu, tolist, precision_recall_curve, auc, std, get_negative_positive_query_set, test_predictions, size, softmax, detach, reshape, squeeze, Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d, Linear, normal_,sigmoid,relu,zero_,fill_,sqrt,get_device_properties, interpolate, repeat, unsqueeze, transpose, weights_init, init, ones, zeros,main, extend, clip_grad_norm_, encoder},          % Custom highlighting
	emphstyle=\color{ipython_purple}\ttfamily,    % Custom highlighting style
}


\usetheme{Antibes}
\usecolortheme{default} %https://hartwork.org/beamer-theme-matrix/
%\useoutertheme[right,color=red]{sidebar} %{infolines}
%\setbeamertemplate{sidebar canvas right}[vertical shading][top=red,bottom=white]
\setbeamercovered{dynamic}

\begin{document}
	\begin{frame}
		\maketitle
	\end{frame}

\begin{frame}
	\frametitle{Contenuti}
	\tableofcontents
\end{frame}

\section{Introduzione}
\begin{frame}
	\frametitle{Sound Event Detection}
	\begin{block}{Obiettivo}
	Individuazione di eventi sonori percettivamente simili all'interno di una registrazione.	
	\end{block}
	
	Esempi di applicazioni:
	\begin{itemize}
	\item rilevazione di particolari suoni nella musica
	\item rimozione di parole di riempimento nei podcast
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Approccio proposto}
	Modelli classici di deep-learning:
	\begin{itemize}
	\item grande mole di dati per effettuare il training
	\item la rete è capace di riconoscere solo eventi sonori su cui è stata allenata.
	\end{itemize}
	
	\begin{block}{Approccio \textit{few-shot}}
		\begin{itemize}
		\item la rete assumerà la capacità di riconoscere la somiglianza fra due suoni che sta analizzando
		\item la rete ha la capacità di riconoscere una parola non vista in fase di training
		\end{itemize}
	\end{block}
\end{frame}

\section{Few-shot learning}
\begin{frame}
	\frametitle{Few-shot learning}
	L'algoritmo di few-shot learning non necessita di un dataset numeroso per il training.
	\begin{block}{Meta-learning}
	L'apprendimento supervisionato tradizionale chiede al modello di riconoscere i dati con cui la rete è stata allenata. Diversamente, l'obiettivo del meta apprendimento è \textit{imparare come imparare} a classificare i dati durante il training e generalizzare quanto imparato durante il test.
	\end{block}
	 In ogni episodio di training della rete, le parole da riconoscere sono diverse, generalizzando il più possibile l'evento e allenando la capacità della rete di riconoscere la somiglianza tra le parole, ma non le parole stesse.
\end{frame}

\begin{frame}
\begin{block}{Classificazione C-way K-shot}
I modelli di few-shot learning sono allenati per risolvere il
compito di classificazione \textit{C-way K-shot}, dove C è il numero fisso di classi tra cui discriminare e K è il numero di esempi forniti per classe.
\end{block}
La figura è un esempio di un task di classificazione 2-way 5-shot, in cui ogni colore rappresenta una classe.
\begin{figure}[h]
	\centering	
	\includegraphics[width=.7\textwidth]{Immagini/few_shot_learning_model}
	\caption{Modello few-shot learning nel caso 5\emph{-way} 2\emph{-shot}}
\end{figure}
\end{frame}

\begin{frame}
	\frametitle{C-way K-shot}
	\begin{block}{Training}
		Per la classificazione C-way K-shot, ogni episodio di training usa C classi con K esempi ciascuna. Questi sono chiamati \textit{support set} e vengono utilizzati per apprendere come risolvere il task. Inoltre, esistono ulteriori esempi delle stesse	classi, note come \textit{query set}, utilizzate per valutare le prestazioni dell'episodio corrente.
	\end{block}
	
	Durante il training, la funzione costo valuta le prestazioni sul query set, dato il rispettivo support set.
	
	\begin{block}{Test}
	Durante il test, utilizziamo un insieme di immagini completamente diverse
	e valutiamo le prestazioni sul query set, dato il support set.
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Training e test task nel caso 3-way 2-shot}
	\begin{figure}[h]
		\centering	
		\includegraphics[width=\textwidth]{Immagini/training_task}
		\caption{Esempio 3-way-2-shot.}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Metodo proposto per il few-shot sound event detection}
	Il modello few-shot viene applicato	ad un insieme aperto. Inoltre, viene costruito un set di esempi negativi e positivi per incrementare la precisione senza bisogno di ulteriore sforzo umano nell'etichettatura dei dati.
	\begin{figure}[h]
	\includegraphics[width=.5\textwidth]{Immagini/few_shot_sound_event_detection_method}
	\caption{Metodo proposto per il few-shot sound event detection. (a) Applicazione del modello few-shot, (b) costruzione del set di esempi negativi, in blu, e (c) data augmentation per la generazioni di più esempi positivi, in arancione.}
	\end{figure}
\end{frame}



\section{Preprocessing del Dataset}
\begin{frame}
	\frametitle{Dataset Spoken Wikipedia Corpora}
	\begin{block}{Spoken Wikipedia Corpora}
	\begin{itemize}
	\item nato da lettori volontari di articoli di Wikipedia
	\item formato da articoli in inglese, tedesco e olandese per gli utenti che non sono in grado di leggere la versione scritta dell'articolo. 
	\item  i file audio sono raccolti in un corpus, cioè una raccolta
		ordinata e completa di opere o di autori, allineato nel tempo, rendendolo
		accessibile per la ricerca.
	\end{itemize}
	\end{block}
	Ogni cartella del dataset corrisponde ad un articolo di Wikipedia e in ognuna di esse è contenuto un file ``aligned.swc'' in cui sono salvate tutte le informazioni sull'audio come il nome del lettore, le parole e i relativi timestamp.
\end{frame}

\begin{frame}
	Fasi del preprocessing:
	\begin{enumerate}
	\item Individuazione dei lettori con i relativi audio
	\item Individuazione delle parole con almeno 10 istanze negli audio 
	\item Associazione delle parole per ogni lettore
	\item Divisione in lettori di training, validation e test
	\item Salvataggio dei rispettivi dataset considerando un massimo di 64 istanze per 32 classi 
	\end{enumerate}
\end{frame}

\begin{frame}
	Il dataset Spoken Wikipedia Corpora contiene un totale di 1340 audio di
	diversi lettori, ma nel progetto vengono presi solo gli audio che contengono
	annotazioni a livello di parola.
	
	Vengono salvati i nomi dei lettori che pronunciano almeno 2 parole per 26 volte.
	
	\begin{block}{Partizionamento dei lettori}
	Vengono presi 208 lettori e partizionati con un rapporto 138 : 15 : 30 tra lettori	di training, validation e test.
	\end{block}
	
	Si effettua un controllo sul nome del lettore perché può capitare che questo venga salvato nel file ``aligned.swc'' in modi diversi. Ad esempio, in alcuni file si può trovare ``:en:user:alexkillby|alexkillby'', mentre in altri
	solo ``alexkillby''.
\end{frame}

\begin{frame}
	Per ogni cartella del dataset SWC si esegue il parsing del file XML, iterando l'albero fino al	tag ``n'', cioè fino al tag che contiene la normalizzazione della parola. Se è presente la chiave ``start'' (o, in modo equivalente,
		``end''), la parola viene salvata.
	\begin{block}{Target words}
	Le target words sono le parole che si ripetono almeno 10 volte nel testo.
	\end{block}
	Vengono salvati due json all'interno di ogni cartella del dataset:
	\begin{itemize}
	\item ``word\_count.json'' contiene tutte le parole pronunciate almeno 10 volte
	\item ``target\_words.json'' contiene al massimo 10 target words utilizzate per il test.
	
	\end{itemize}
	Se ci sono più di 10 parole che soddisfano la condizione di target word, se ne prendono solo 10 scelte in modo random. In questo modo, si evita di scegliere parole molto comuni o molto rare.
\end{frame}

\begin{frame}
	Per ogni istanza delle parole, prendiamo una finestra di mezzo secondo centrata
	sulla parola, calcoliamo lo spettrogramma mel da 128 bin e lo portiamo in
	scala logaritmica.
	\begin{block}{Spettrogramma}
		Lo spettrogramma rappresenta l'intensità di un suono in funzione del tempo e della frequenza.
	\end{block}
	\begin{block}{Scala mel}
		La scala mel è una scala di percezione dell'altezza (pitch) di un suono.
			La relazione tra la scala mel e quella comunemente usata è rappresentata
			dall'uguaglianza tra 1000 Mel e 1000 Hz all'intensità di 40 dB.
	\end{block}
\end{frame}

\begin{frame}
	\begin{figure}[h]
		\centering	
		\includegraphics[width=1\textwidth]{../Stones_spectrogram}
		\caption{Spettrogramma di mezzo secondo centrato sulla parola ``stones'' pronunciata nell'audio ``I can't get no satisfaction''}
		\label{fig:stones_spec}
	\end{figure}
\end{frame}

\section{Prototypical Network}
\begin{frame}
	\frametitle{Prototypical Network}
	\begin{block}{Prototipo}
	L'approccio si basa sull'idea che esista un embedding in cui i punti delle istanze di una classe si raggruppano attorno a una singola rappresentazione per ogni classe, chiamata \textbf{prototipo}. 
	\end{block}	
	Nella classificazione few-shot per ogni episodio viene fornito un support set di $N$ esempi etichettati  $S=\{(\mathbf{x}_1,y_1), \dots,(\mathbf{x}_N,y_N)\}$ dove $\mathbf{x}_i\in \mathbb{R}^D$ rappresenta il vettore $D$-dimensionale della feature (nel nostro caso spettrogrammi di dimensione $128 \times 51$) e $y_i \in \{1, \dots, K\}$ la rispettiva label. $S_k$ denota il set di esempi etichettati con la classe $k$.
\end{frame}
\begin{frame}
\begin{figure}[h]
	\centering
	{\includegraphics[width=0.5\textwidth]{Immagini/proto_few_shot}}
	\caption{I prototipi few-shot $\mathbf{c}_k$ sono calcolati come la media degli embedding del support set per ogni classe. I punti degli embedding delle query sono classificati facendo il softmax sulle distanze del prototipo delle classi: $p_\phi(y = k|\mathbf{x}) \propto \exp \left(-d(f_\phi(\mathbf{x}),\mathbf{c}_k) \right)$.}
\end{figure}
\end{frame}

\begin{frame}
	La rete Prototypical calcola una rappresentazione $M$-dimensionale $\mathbf{c}_k$, o \emph{prototipo}, di ogni classe tramite una funzione di embedding $f_\phi : \mathbb{R}^D \rightarrow \mathbb{R}^M$ con parametri da allenare $\phi$. La funzione di embedding è rappresentata da una rete convoluzionale. Ogni prototipo è calcolato come la media tra gli embedding delle istanze della stessa classe.
	\begin{equation}
		\mathbf{c}_k=\frac{1}{|S_k|}\sum_{(\mathbf{x}_i,y_i)\in S_k}f_\phi (\mathbf{x}_i)
	\end{equation}
\end{frame}

\begin{frame}
Data una funzione distanza $d: \mathbb{R}^M \times \mathbb{R}^M \rightarrow [0, +\infty )$, la rete Prototypical calcola la relazione di una query $\mathbf{x}$ rispetto ai prototipi tramite la funzione softmax delle distanze prese con segno negativo.
	\begin{equation}\label{eq:softmax}
	p_\phi(y=k|\mathbf{x})=\dfrac{\exp(-d(f_\phi(\mathbf{x}),\mathbf{c}_k))}{\sum_k' \exp(-d(f_\phi(\mathbf{x}),\mathbf{c}_k'))}
	\end{equation}
	
	Il processo di training avviene minimizzando il negativo del logaritmo della probabilità
	\begin{equation}\label{eq:loss_proto}
		J(\phi)=-\log\left(p_\phi(y=k|\mathbf{x})\right)
	\end{equation}
	considerando la distanza fra query e il prototipo della sua classe.
\end{frame}

\begin{frame}
	\frametitle{Architettura della rete}La CNN che funge da rete di embedding per la Prototypical Network è costituita da 4 strati convoluzionali. Il primo di essi ha come ingresso un singolo canale e come uscita 64 mentre i tre successivi traspongono 64 canali in altri 64.

	Al termine dei blocchi viene effettuato il reshape dell'uscita, \texttt{x.view(x.size(0),-1)}, in modo da avere una matrice le cui righe rappresentano gli embedding vettoriali.
	
	\begin{figure}[h]
		\centering	
		\includegraphics[width=\textwidth]{Immagini/proto_steps}
		\caption{Fasi della Prototypical Network.}
	\end{figure}
\end{frame}

\begin{frame}[fragile]
\begin{lstlisting}[language=iPython,firstnumber=1, caption=protonet.py, label= Protonet,captionpos=b]
import torch.nn as nn

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def conv_block(in_channels,out_channels):

    return nn.Sequential(
        nn.Conv2d(in_channels,out_channels,3,padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(),
        nn.MaxPool2d(2)
    )

class Protonet(nn.Module):
    def __init__(self):
        super(Protonet,self).__init__()
        self.encoder = nn.Sequential(
            conv_block(1,64),
            conv_block(64,64),
            conv_block(64,64),
            conv_block(64,64)
        )
    def forward(self,x):
        (num_samples,mel_bins, seq_len) = x.shape
        x = x.view(-1,1,mel_bins,seq_len) 
        x = self.encoder(x)
        return x.view(x.size(0),-1)
\end{lstlisting}
\end{frame}
\begin{frame}
	\frametitle{Blocco convoluzionale}
		Ogni blocco convoluzionale ha 4 fasi:
		\begin{itemize}
			\item Convoluzione bidimensionale con un kernel $3 \times 3$ i cui parametri vengono aggiornati ad ogni backward. Viene effettuato un padding di zeri ai bordi dell'ingresso.
			\item Batch normalization: un metodo utilizzato per rendere le reti neurali artificiali più veloci e stabili attraverso la normalizzazione degli input dei livelli con re-centering and re-scaling.
			\item ReLu: il rettificatore è una funzione di attivazione definita come la parte positiva del suo argomento. $f(x)=\max(0,x)$
			\item Max-pooling: metodo per ridurre la dimensione di un'immagine, suddividendola in blocchi e tenendo solo quello col valore più alto.
		\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Training}
	A partire dalla lista di lettori di training/validation, vengono selezionati quelli che hanno almeno un numero di parole diverse pari a C e li suddivide tra training e validation seguendo lo stesso rapporto usato dal paper.
	Il numero totale di iterazioni è 60000. Per ogni episodio:
	\begin{enumerate}
	\item Si campiona un lettore casuale e si selezionano C parole pronunciate da esso.
	\item Si formano il support set e il query set di dimensioni $C \times K \times 128 \times 51$ e $C \times Q \times 128 \times 51$, rispettivamente.
	\item Si calcola la loss relativa all'episodio
	\item Tramite la backpropagation si aggiornano i parametri della rete in base alla loss.
	\end{enumerate}
\end{frame}
\begin{frame}
	Per il calcolo della loss le operazioni sono:
	\begin{enumerate}
	\item Entrambi i set vengono concatenati, risultando in un tensore $\left(C \cdot (Q + K)\right) \times 128 \times 51$, per poter essere passate al modello che calcolerà gli embedding per ogni istanza.
	\item Gli elementi del support set che fanno parte della stessa classe andranno a costituire i prototipi tramite una media dei loro valori.
	\item Vengono successivamente calcolate le distanze rispetto ai prototipi degli embedding di ogni classe ottenendo una matrice di dimensioni $(C \cdot Q) \times C$. L'i-esima riga e la j-esima colonna rappresentano la distanza euclidea tra gli embedding della i-esima query ($i = 1, \dots, C \cdot Q$) e quelli del j-esimo ($j = 1, \dots, C$) prototipo.
	\item Per ogni query viene poi usata la funzione di attivazione softmax, i cui argomenti sono le distanze tra la query e i prototipi delle varie classi. La funzione loss da minimizzare è il logaritmo del softmax preso con segno negativo.
	\end{enumerate}
\end{frame}

\begin{frame}
	\frametitle{Validation}
	Ogni 5000 episodi del training vengono effettuati 1000 episodi di validation.
	Come nel training vengono ricavati support set e query set, ma in questo caso dai lettori di validation.
	Per questi vengono calcolati allo stesso modo loss e accuracy e vengono salvati.
	In questo caso però i parametri del modello non vengono aggiornati.
	Questo processo è necessario per verificare l'effettività del modello su ingressi mai visti in fase di training.
	\begin{block}{Criterio di salvataggio del modello}
	Al termine dei 1000 episodi di validation, se la media dell'accuracy è migliore di quella calcolata nel passo precedente il modello viene salvato e sovrascritto, altrimenti viene ignorato.
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Test}
	L'oggetto del test non sono i lettori, ma gli audio. Per ognuno di essi, ad ogni iterazione, vengono estratti:
	\begin{itemize}
	\item \textbf{Positive set}: contiene gli esempi della parola da individuare nell'audio.
	\item \textbf{Negative set}: contiene istanze di parole diverse da quella target. Rappresenta un generico esempio di tutto ciò che non comprende la parola da cercare.
	\item \textbf{Query set}: campioni che la rete deve classificare correttamente.
	\end{itemize}
\end{frame}

\begin{frame}
Analogamente al C-way K-shot, positive e negative set rappresentano le due classi del support set con cui confrontare il query set.

\begin{block}{Calcolo delle predizioni}
La somiglianza tra gli embedding della query e i due prototipi è data dal softmax delle distanze con segno negativo. Se il softmax tende a zero, significa che la query è molto distante dal prototipo, viceversa, se tende a 1, è molto vicina.
\end{block}

Per valutare le prestazioni si confrontano le label della query con le predizioni associata al positive set, calcolando l'AUPRC.
\end{frame}

\section{Relation Network}
\begin{frame}
	\frametitle{Relation Network}
	\begin{block}{Architettura Relation Network}
	La Relation Network è costituita da due moduli: 
	\begin{itemize}
	\item un modulo di \emph{embedding} $f_\varphi$ (equivalente a quello nella Prototypical)
	\item un modulo di \emph{relation} $g_\phi$.
	\end{itemize} 
	\end{block}
	Le istanze $x_i$ del query set $\mathcal{Q}$ e quelle $x_j$ del support set $\mathcal{S}$ vengono date in ingresso al modulo di embedding producendo dei vettori (feature maps) $f_\varphi(x_i)$ e $f_\varphi(x_j)$.
	Questi ultimi vengono poi dati all'operatore $\mathcal{C}(\cdot ,\cdot)$ che ne fa la concatenazione: $\mathcal{C}(f_\varphi(x_i),f_\varphi(x_j))$.
	
	La rete decisionale viene aggiornata in modo che una concatenazione di elementi simili restituisca un risultato vicino a 1.
\end{frame}
\begin{frame}
	Nel caso \textit{C-way K-shot}, la query viene concatenata con la somma elemento per elemento degli embedding di ogni istanza delle classi producendo $C$ punteggi di somiglianza.
	\begin{equation}
		r_{i,j}=g_\phi(\mathcal{C}(f_\varphi(x_i),f_\varphi(x_j))),  \qquad i = 1, 2, \dots, C
	\end{equation}
	Per allenare il modello viene usato l'errore quadratico medio (MSE) in modo che l'uscita del modulo di decisione produca 1 se i vettori concatenati sono della stessa classe e 0 altrimenti.
\end{frame}

\begin{frame}
\frametitle{Relation Network}
\begin{figure}[h]
	\centering	
	\includegraphics[width=\textwidth]{Immagini/relation_network}
	\caption{Architettura della Relation Network nel caso 5\emph{-way} 1\emph{-shot} con un esempio di query.}
\end{figure}
\end{frame}

\begin{frame}[fragile]
\begin{lstlisting}[language=iPython,firstnumber=1, caption=relation\_network.py, label=relation_encoder,captionpos=b]
class CNNEncoder(nn.Module):
    def __init__(self):
        super(CNNEncoder, self).__init__()
        self.layer1 = nn.Sequential(
                        nn.Conv2d(1,64,kernel_size=3,padding=0),
                        nn.BatchNorm2d(64, momentum=1, affine=True),
                        nn.ReLU(),
                        nn.MaxPool2d(2))
        self.layer2 = nn.Sequential(
                        nn.Conv2d(64,64,kernel_size=3,padding=0),
                        nn.BatchNorm2d(64, momentum=1, affine=True),
                        nn.ReLU(),
                        nn.MaxPool2d(2))
        self.layer3 = nn.Sequential(
                        nn.Conv2d(64,64,kernel_size=3,padding=1),
                        nn.BatchNorm2d(64, momentum=1, affine=True),
                        nn.ReLU(),
                        nn.MaxPool2d(2))
        self.layer4 = nn.Sequential(
                        nn.Conv2d(64,64,kernel_size=3,padding=1),
                        nn.BatchNorm2d(64, momentum=1, affine=True),
                        nn.ReLU())
                        #nn.MaxPool2d(2))

    def forward(self,x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        return out # 64
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\begin{lstlisting}[language=iPython,firstnumber=1, caption=relation\_network.py, label=relation_network,captionpos=b]
class RelationNetwork(nn.Module):
    def __init__(self,input_size,hidden_size):
        super(RelationNetwork, self).__init__()
        self.layer1 = nn.Sequential(
                        nn.Conv2d(128,64,kernel_size=3,padding=1),
                        nn.BatchNorm2d(64, momentum=1, affine=True),
                        nn.ReLU(),
                        nn.MaxPool2d(2))
        self.layer2 = nn.Sequential(
                        nn.Conv2d(64,64,kernel_size=3,padding=1),
                        nn.BatchNorm2d(64, momentum=1, affine=True),
                        nn.ReLU(),
                        nn.MaxPool2d(2))
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)

    def forward(self,x):
        out = self.layer1(x)    #  (CLASS_NUM * BATCH_NUM_PER_CLASS * CLASS_NUM) X FEATURE_DIM X 2 X 2
        out = self.layer2(out)  #  (CLASS_NUM * BATCH_NUM_PER_CLASS * CLASS_NUM) X FEATURE_DIM X 1 X 1
        out = out.view(out.size(0),-1)   #  (CLASS_NUM * BATCH_NUM_PER_CLASS * CLASS_NUM) X FEATURE_DIM
        out = F.relu(self.fc1(out))      #  (CLASS_NUM * BATCH_NUM_PER_CLASS * CLASS_NUM) X RELATION_DIM
        #out = F.sigmoid(self.fc2(out)) # deprecated
        out = torch.sigmoid(self.fc2(out))  #  (CLASS_NUM * BATCH_NUM_PER_CLASS * CLASS_NUM) X 1
        return out
\end{lstlisting}
\end{frame}   
    
\begin{frame}    
Come nella Prototypical Network, anche nella Relation Network viene
usata una architettura con quattro blocchi convoluzionali per il modulo di
\textbf{embedding}.
Ogni blocco convoluzionale contiene:
\begin{itemize}
\item una convoluzione con 64 filtri $3 \times 3$,
\item una batch normalization,
\item un layer ReLU non lineare.
\end{itemize} 
I primi tre blocchi contengono anche un layer di max pooling $2 \times 2$, mentre l'ultimo no.
\end{frame}

\begin{frame}
Il modulo \textbf{relation} è costituito da due blocchi convoluzionali e due layer
fully-connected. Ciascuno dei blocchi convoluzionali è composto da:
\begin{itemize}
\item una convoluzione $3 \times 3$ con 64 filtri, 
\item una batch normalization,
\item un layer ReLU non lineare,
\item un max pooling $2 \times 2$.
\end{itemize}

Tutti i layer fully-connected sono ReLU eccetto quello di output, composto
da una sigmoide per generare punteggi di relazione in un intervallo compreso
tra 0 e 1.
\end{frame}   

\begin{frame}
	\frametitle{Training}
	Come nella Prototypical Network, le fasi sono:
	\begin{enumerate}
	\item Si campiona un lettore casuale e si selezionano C parole pronunciate da esso.
	\item Si formano il support set e il query set di dimensioni $C \times K \times 128 \times 51$ e $C \times Q \times 128 \times 51$, rispettivamente.
	\item Si calcola la loss relativa all'episodio
	\item Tramite la backpropagation si aggiornano i parametri della rete in base alla loss.
	\end{enumerate}
	La differenza consiste nel calcolo della loss poiché la metrica non è più fissa, ma con parametri allenabili.
\end{frame}
\begin{frame}
	\frametitle{Validation}
\end{frame}
\begin{frame}
	\frametitle{Test}
	Inserire calcolo AUPRC
\end{frame}

\section{Risultati}
\begin{frame}
	\frametitle{Matrice di confusione}
	Nella fase di test, la rete elabora una predizione dell'output a partire da un ingresso noto che poi viene confrontata con il valore effettivo.
	Nel nostro caso è presente un positive set e un negative set, si tratta quindi di classificazione binaria.
	Il confronto tra predizione e label può produrre quattro risultati:
	\begin{itemize}
		\item Vero Negativo (TN): il valore reale è negativo e il valore predetto è negativo;
		\item Vero Positivo (TP): il valore reale è positivo e il valore predetto	è positivo;
		\item Falso Negativo (FN): il valore reale è positivo e il valore predetto è negativo;
		\item Falso Positivo (FP): il valore reale è negativo e il valore predetto	è positivo;
	\end{itemize}
\end{frame}
\begin{frame}
	\frametitle{Precision e Recall}
	\begin{block}{Precision}
	Il parametro Precision rappresenta quanti tra i casi predetti come positivi sono realmente positivi.
	\end{block}
	\begin{block}{Recall}
	Il parametro Recall indica quanti tra i casi realmente positivi è stato predetto in modo corretto.
	\end{block}
	\begin{equation}
	Precision=\frac{TP}{TP+FP}
	\end{equation}
	\begin{equation}
	Recall=\frac{TP}{TP+FN}
	\end{equation}
\end{frame}
\begin{frame}[fragile]
	\frametitle{AUPRC}
	\begin{block}{Area under precision-recall curve (AUPRC)}
	Area sottessa dalla curva precision-recall.
	Questo valore è molto utile quando i dati sono sbilanciati e, in una classificazione binaria, siamo più interessati al riconoscimento di una classe in particolare.
	\end{block}
	Per calcolare questo parametro sono state usate le funzioni \texttt{precision\_recall\_curve} e \texttt{sklearn.metrics.auc}.
	\begin{lstlisting}[language=iPython,firstnumber=1, caption=calcolo della AUPRC per ogni audio, label= auc_tmp,captionpos=b]
	precision, recall, thresholds = sklearn.metrics.precision_recall_curve (y_true, y_pred)
	auc_tmp = sklearn.metrics.auc(recall, precision)	
	auc_list.append(auc_tmp)
	\end{lstlisting}
	Infine si fa una media di tutte le AUPRC di ogni audio di test.
\end{frame}
\begin{frame}
	\frametitle{Calcolo AUPRC}
	\begin{enumerate}
	\item Consideriamo i valori di probabilità delle query di appartenere alla classe positiva.
	\item Ordiniamo il vettore in ordine decrescente.
	\item Consideriamo come soglia il valore di probabilità presente al primo elemento e calcoliamo precision e recall confrontando con un vettore che indica la classe corretta.
	\item Spostiamo la soglia al valore del secondo elemento e calcoliamo un'altra coppia di parametri.
	\item Ripetiamo il passo 3 e 4 per tutti gli elementi.
	\item Otteniamo una curva considerando alle ascisse i valori di recall e come ordinate i valori di precision.
	\item Calcolando l'area sottesa dalla curva ricaviamo il parametro AUPRC.
	\end{enumerate}

\end{frame}

\begin{frame}
\begin{figure}[h]
	\centering	
	\includegraphics[width=0.8\textwidth]{Immagini/pre_rec_2}
	\caption{Curva precision-recall per il test della rete prototypical con $C = 2$, $K=1$ e $p = 5$.}
\end{figure}
\end{frame}
\begin{frame}
\frametitle{Risultati}
\begin{columns}
\column{0.5\textwidth}
\begin{table}
\begin{tabular}{ccc}
\toprule
\textbf{C, K} & \textbf{AUPRC} & \textbf{\begin{tabular}[c]{@{}c@{}}Deviazione \\ standard\end{tabular}} \\ 
\midrule
2, 1          & 0.731        & 0.069                        \\
2, 5          & 0.721        & 0.074                        \\
5, 1          & 0.709        & 0.089                        \\
5, 5          & 0.688        & 0.062                        \\
10, 1         & 0.728        & 0.067                        \\
10, 5         & 0.727        & 0.078                        \\
10, 10        & 0.725        & 0.059                        \\ 
\bottomrule

\end{tabular}
\caption{Prototypical network $p=1$, $n=10$.}
\end{table}


\column{0.5\textwidth}
\begin{table}
\begin{tabular}{ccc}
\toprule
\textbf{C, K} & \textbf{AUPRC} & \textbf{\begin{tabular}[c]{@{}c@{}}Deviazione \\ standard\end{tabular}} \\ 
\midrule
2, 1          & 0.800        & 0.056                        \\
2, 5          & 0.791        & 0.062                        \\
5, 1          & 0.774        & 0.092                        \\
5, 5          & 0.754        & 0.075                        \\
10, 1         & 0.787        & 0.057                        \\
10, 5         & 0.794        & 0.070                        \\
10, 10        & 0.793        & 0.050                        \\ 
\bottomrule

\end{tabular}
\caption{Prototypical network $p=5$, $n=10$.}
\end{table}

\end{columns}

\end{frame}


\begin{frame}
\begin{columns}
\column{0.5\textwidth}
\begin{table}
\begin{tabular}{ccc}
\toprule
\textbf{C, K} & \textbf{AUPRC} & \textbf{\begin{tabular}[c]{@{}c@{}}Deviazione \\ standard\end{tabular}} \\ 
\midrule
2, 1          & 0.775        & 0.069                        \\
2, 5          & 0.609        & 0.046                        \\
5, 1          & 0.889        & 0.061                        \\
5, 5          & 0.666        & 0.047                        \\
10, 1         & 0.898        & 0.053                        \\
10, 5         & 0.710        & 0.059                        \\
10, 10        & 0.538        & 0.015                        \\ 
\bottomrule

\end{tabular}
\caption{Relation network $p=1$, $n=10$.}
\end{table}


\column{0.5\textwidth}
\begin{table}
\begin{tabular}{ccc}
\toprule
\textbf{C, K} & \textbf{AUPRC} & \textbf{\begin{tabular}[c]{@{}c@{}}Deviazione \\ standard\end{tabular}} \\ 
\midrule
2, 1          & 0.596        & 0.042                        \\
2, 5          & 0.813        & 0.056                        \\
5, 1          & 0.698        & 0.050                        \\
5, 5          & 0.945        & 0.037                        \\
10, 1         & 0.691        & 0.052                        \\
10, 5         & 0.945        & 0.036                        \\
10, 10        & 0.845        & 0.061                        \\ 
\bottomrule

\end{tabular}
\caption{Relation network $p=5$, $n=10$.}
\end{table}
\end{columns}
\end{frame}

\begin{frame}
\begin{figure}[h]
	\centering	
	\includegraphics[width=1\textwidth]{Immagini/results}
	\caption{Risultati ottenuti.}
\end{figure}
\end{frame}


\section{Conclusioni}
\begin{frame}
	\frametitle{Conclusioni}
	Si può notare che:
	\begin{itemize}
	\item La rete Prototypical ha un AUPRC più grande con $p = 5$ rispetto a $p = 1$ per ognuno dei modelli allenati.
	\item Il miglioramento nell'AUPRC è di circa 6-7\% passando da $p = 1$ a $p = 5$.
	\item Per la rete Relation questo comportamento non si presenta.
	\item Con $p = 1$ si nota che la rete Prototypical ha un risultato migliore per $(C, K) \in \{ (10, 5), (10, 10)\}$, mentre nei restanti casi la rete Relation risulta migliore.
	\item Per $p = 5$ la rete Prototypical ha invece un AUPRC più elevato per $(C, K) \in \{ (2, 1), (5, 1), (10, 1)\}$.
	\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}[h]
	\centering	
	\includegraphics[width=1\textwidth]{Immagini/results_paper}
	\caption{Risultati proposti nel paper}
\end{figure}
\end{frame}
\begin{frame}
	\frametitle{Confronto}
	Confrontando i risultati con quelli del paper si può osservare che:
	\begin{itemize}
	\item in entrambi i grafici, per la rete Prototypical con $C = 10$ non si ha un miglioramento significativo dell'AUPRC e che questo tipo di rete si comporta meglio con $p = 5$.
	\item la rete Relation ha sempre dei risultati peggiori rispetto alla Prototypical, mentre nel nostro caso dipende dai valori di $C$ e $K$ con cui è stata allenata la rete. 
	\item i valori di AUPRC nella nostra implementazione sono generalmente più alti
	\end{itemize}
\end{frame}

\begin{frame}
	Possibili cause di divergenza dei risultati:
	\begin{itemize}
	\item il preprocessing del dataset.
	\item diversa implementazione della rete Relation, la cui funzione \textit{g\ped{sim}} non era stata descritta nell'articolo.
	\item differente criterio per il salvataggio dei modelli durante il training.
	\end{itemize}
\end{frame}

\end{document}